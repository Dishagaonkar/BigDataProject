{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1739030e-b68c-4493-b9f2-c8c0a4e822a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/jovyan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 23:55:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/07 23:55:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    FloatType,\n",
    "    DoubleType,\n",
    "    ShortType,\n",
    "    DecimalType,\n",
    ")\n",
    "\n",
    "os.chdir(\"..\")\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "INDEX_PATH = DATA_DIR / \"dataset_index.csv\"\n",
    "\n",
    "CAT_PROFILE_DIR = BASE_DIR / \"profiles\" / \"categorical\"\n",
    "CAT_PROFILE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"NYC_Categorical_Profiling\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "index_df = pd.read_csv(INDEX_PATH)\n",
    "index_records = index_df.to_dict(orient=\"records\")\n",
    "len(index_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e014469-e26e-4b30-9415-f9c2959b8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_types = (\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    FloatType,\n",
    "    DoubleType,\n",
    "    ShortType,\n",
    "    DecimalType,\n",
    ")\n",
    "\n",
    "def get_categorical_columns(sdf, max_distinct_numeric=20):\n",
    "    string_cols = []\n",
    "    numeric_low_card = []\n",
    "\n",
    "    for field in sdf.schema.fields:\n",
    "        if isinstance(field.dataType, StringType):\n",
    "            string_cols.append(field.name)\n",
    "\n",
    "    for field in sdf.schema.fields:\n",
    "        if isinstance(field.dataType, numeric_types):\n",
    "            col = field.name\n",
    "            try:\n",
    "                distinct_cnt = sdf.select(col).distinct().count()\n",
    "            except Exception:\n",
    "                distinct_cnt = None\n",
    "            if distinct_cnt is not None and distinct_cnt <= max_distinct_numeric:\n",
    "                numeric_low_card.append(col)\n",
    "\n",
    "    cols = sorted(set(string_cols + numeric_low_card))\n",
    "    return cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79804ccf-aa0f-4600-9d9c-d4ca4c881e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-12-07 23:55:45.925\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `SAT Critical Reading Avg`.` Score` cannot be resolved. Did you mean one of the following? [`SAT Critical Reading Avg. Score`, `SAT Writing Avg. Score`, `SAT Math Avg. Score`, `Num of SAT Test Takers`, `SCHOOL NAME`]. SQLSTATE: 42703\", \"context\": {\"file\": \"line 40 in cell [3]\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o33.withColumn.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `SAT Critical Reading Avg`.` Score` cannot be resolved. Did you mean one of the following? [`SAT Critical Reading Avg. Score`, `SAT Writing Avg. Score`, `SAT Math Avg. Score`, `Num of SAT Test Takers`, `SCHOOL NAME`]. SQLSTATE: 42703;\\n'Project [DBN#17, SCHOOL NAME#18, Num of SAT Test Takers#19, 'lower('trim('SAT Critical Reading Avg. Score)) AS SAT Critical Reading Avg. Score#82, SAT Math Avg. Score#21, SAT Writing Avg. Score#22]\\n+- Relation [DBN#17,SCHOOL NAME#18,Num of SAT Test Takers#19,SAT Critical Reading Avg. Score#20,SAT Math Avg. Score#21,SAT Writing Avg. Score#22] csv\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:1283)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:232)\\n\\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2187)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:1819)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 23 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/opt/conda/envs/bigdata/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/opt/conda/envs/bigdata/lib/python3.12/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `SAT Critical Reading Avg`.` Score` cannot be resolved. Did you mean one of the following? [`SAT Critical Reading Avg. Score`, `SAT Writing Avg. Score`, `SAT Math Avg. Score`, `Num of SAT Test Takers`, `SCHOOL NAME`]. SQLSTATE: 42703;\n'Project [DBN#17, SCHOOL NAME#18, Num of SAT Test Takers#19, 'lower('trim('SAT Critical Reading Avg. Score)) AS SAT Critical Reading Avg. Score#82, SAT Math Avg. Score#21, SAT Writing Avg. Score#22]\n+- Relation [DBN#17,SCHOOL NAME#18,Num of SAT Test Takers#19,SAT Critical Reading Avg. Score#20,SAT Math Avg. Score#21,SAT Writing Avg. Score#22] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_cols:\n\u001b[32m     40\u001b[39m     norm_col = F.lower(F.trim(F.col(col)))\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     df_norm = \u001b[43msdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     dist = (\n\u001b[32m     44\u001b[39m         df_norm.groupBy(col)\n\u001b[32m     45\u001b[39m         .agg(F.count(\u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     46\u001b[39m         .orderBy(F.desc(\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     47\u001b[39m     )\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/bigdata/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:1623\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   1618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m   1619\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1620\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1621\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1622\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1623\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/bigdata/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/bigdata/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `SAT Critical Reading Avg`.` Score` cannot be resolved. Did you mean one of the following? [`SAT Critical Reading Avg. Score`, `SAT Writing Avg. Score`, `SAT Math Avg. Score`, `Num of SAT Test Takers`, `SCHOOL NAME`]. SQLSTATE: 42703;\n'Project [DBN#17, SCHOOL NAME#18, Num of SAT Test Takers#19, 'lower('trim('SAT Critical Reading Avg. Score)) AS SAT Critical Reading Avg. Score#82, SAT Math Avg. Score#21, SAT Writing Avg. Score#22]\n+- Relation [DBN#17,SCHOOL NAME#18,Num of SAT Test Takers#19,SAT Critical Reading Avg. Score#20,SAT Math Avg. Score#21,SAT Writing Avg. Score#22] csv\n"
     ]
    }
   ],
   "source": [
    "profiling_stats = []\n",
    "\n",
    "for row in index_records:\n",
    "    if row.get(\"download_status\") != \"ok\":\n",
    "        continue\n",
    "\n",
    "    dataset_id = row[\"dataset_id\"]\n",
    "    local_path = row[\"local_path\"]\n",
    "    full_path = str(BASE_DIR / local_path)\n",
    "\n",
    "    start = time.time()\n",
    "    status = \"ok\"\n",
    "    error = None\n",
    "\n",
    "    try:\n",
    "        sdf = (\n",
    "            spark.read\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .csv(full_path)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        status = \"read_error\"\n",
    "        error = str(e)\n",
    "        profiling_stats.append(\n",
    "            {\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"status\": status,\n",
    "                \"error\": error,\n",
    "                \"seconds\": time.time() - start,\n",
    "                \"num_categorical_cols\": 0,\n",
    "            }\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    categorical_cols = get_categorical_columns(sdf)\n",
    "    categorical_profile = {}\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        norm_col = F.lower(F.trim(F.col(col)))\n",
    "        df_norm = sdf.withColumn(col, norm_col)\n",
    "\n",
    "        dist = (\n",
    "            df_norm.groupBy(col)\n",
    "            .agg(F.count(\"*\").alias(\"count\"))\n",
    "            .orderBy(F.desc(\"count\"))\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            total_row = dist.agg(F.sum(\"count\").alias(\"total\")).first()\n",
    "            total = total_row[\"total\"]\n",
    "        except Exception:\n",
    "            total = None\n",
    "\n",
    "        max_values = 100\n",
    "        dist_rows = dist.limit(max_values).collect()\n",
    "\n",
    "        top_values = []\n",
    "        for r in dist_rows:\n",
    "            value = r[col]\n",
    "            count = int(r[\"count\"])\n",
    "            if total:\n",
    "                percent = float(count) / float(total)\n",
    "            else:\n",
    "                percent = None\n",
    "            top_values.append(\n",
    "                {\n",
    "                    \"value\": value,\n",
    "                    \"count\": count,\n",
    "                    \"percent\": percent,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            unique_values = dist.count()\n",
    "        except Exception:\n",
    "            unique_values = len(dist_rows)\n",
    "\n",
    "        categorical_profile[col] = {\n",
    "            \"unique_values\": int(unique_values),\n",
    "            \"top_values\": top_values,\n",
    "        }\n",
    "\n",
    "    feature_groups = {}\n",
    "    for col in sdf.columns:\n",
    "        if \"_\" in col:\n",
    "            prefix = col.split(\"_\")[0]\n",
    "        else:\n",
    "            prefix = col\n",
    "        feature_groups.setdefault(prefix, []).append(col)\n",
    "\n",
    "    out_obj = {\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"categorical_profile\": categorical_profile,\n",
    "        \"feature_groups\": feature_groups,\n",
    "    }\n",
    "\n",
    "    out_path = CAT_PROFILE_DIR / f\"{dataset_id}_categorical.json\"\n",
    "    with out_path.open(\"w\") as f:\n",
    "        json.dump(out_obj, f, indent=2)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    profiling_stats.append(\n",
    "        {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"status\": status,\n",
    "            \"error\": error,\n",
    "            \"seconds\": elapsed,\n",
    "            \"num_categorical_cols\": len(categorical_cols),\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a4740-78b1-4b6f-9d79-9e4658b3765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(profiling_stats)\n",
    "stats_df.to_csv(BASE_DIR / \"profiles\" / \"categorical_profiling_times.csv\", index=False)\n",
    "stats_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

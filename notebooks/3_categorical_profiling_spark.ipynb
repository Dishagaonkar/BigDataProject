{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1739030e-b68c-4493-b9f2-c8c0a4e822a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/jovyan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    FloatType,\n",
    "    DoubleType,\n",
    "    ShortType,\n",
    "    DecimalType,\n",
    ")\n",
    "\n",
    "os.chdir(\"..\")\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "INDEX_PATH = DATA_DIR / \"dataset_index.csv\"\n",
    "\n",
    "CAT_PROFILE_DIR = BASE_DIR / \"profiles\" / \"categorical\"\n",
    "CAT_PROFILE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"NYC_Categorical_Profiling\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "index_df = pd.read_csv(INDEX_PATH)\n",
    "index_records = index_df.to_dict(orient=\"records\")\n",
    "len(index_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e014469-e26e-4b30-9415-f9c2959b8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_types = (\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    FloatType,\n",
    "    DoubleType,\n",
    "    ShortType,\n",
    "    DecimalType,\n",
    ")\n",
    "\n",
    "def get_categorical_columns(sdf, max_distinct_numeric=20):\n",
    "    string_cols = []\n",
    "    numeric_low_card = []\n",
    "\n",
    "    for field in sdf.schema.fields:\n",
    "        if isinstance(field.dataType, StringType):\n",
    "            string_cols.append(field.name)\n",
    "\n",
    "    for field in sdf.schema.fields:\n",
    "        if isinstance(field.dataType, numeric_types):\n",
    "            col = field.name\n",
    "            try:\n",
    "                distinct_cnt = sdf.select(col).distinct().count()\n",
    "            except Exception:\n",
    "                distinct_cnt = None\n",
    "            if distinct_cnt is not None and distinct_cnt <= max_distinct_numeric:\n",
    "                numeric_low_card.append(col)\n",
    "\n",
    "    cols = sorted(set(string_cols + numeric_low_card))\n",
    "    return cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79804ccf-aa0f-4600-9d9c-d4ca4c881e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling categorical columns for f9bf-2cp4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling categorical columns for x3bb-kg5j ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling categorical columns for zt9s-n5aj ...\n",
      "Profiling categorical columns for s3k6-pzi2 ...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: main categorical profiling loop + JSON output + timing\n",
    "\n",
    "profiling_stats = []\n",
    "\n",
    "for row in index_records:\n",
    "    # Skip datasets that didnâ€™t download successfully\n",
    "    if row.get(\"download_status\") != \"ok\":\n",
    "        continue\n",
    "\n",
    "    dataset_id = row[\"dataset_id\"]\n",
    "    local_path = row[\"local_path\"]\n",
    "    full_path = str(BASE_DIR / local_path)\n",
    "\n",
    "    print(f\"Profiling categorical columns for {dataset_id} ...\")\n",
    "\n",
    "    start = time.time()\n",
    "    status = \"ok\"\n",
    "    error = None\n",
    "\n",
    "    # 1) Read CSV with Spark\n",
    "    try:\n",
    "        sdf = (\n",
    "            spark.read\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .csv(full_path)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        status = \"read_error\"\n",
    "        error = str(e)\n",
    "        profiling_stats.append(\n",
    "            {\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"status\": status,\n",
    "                \"error\": error,\n",
    "                \"seconds\": time.time() - start,\n",
    "                \"num_categorical_cols\": 0,\n",
    "            }\n",
    "        )\n",
    "        print(f\"  -> READ ERROR: {error}\")\n",
    "        continue\n",
    "\n",
    "    # 2) Detect categorical columns\n",
    "    categorical_cols = get_categorical_columns(sdf)\n",
    "    categorical_profile = {}\n",
    "\n",
    "    # 3) Build distributions per categorical column\n",
    "    for col in categorical_cols:\n",
    "        # Normalize as lowercase + trimmed string, handling dots/spaces via backticks\n",
    "        norm_expr = F.lower(\n",
    "            F.trim(F.col(f\"`{col}`\").cast(\"string\"))\n",
    "        ).alias(\"value_norm\")\n",
    "\n",
    "        dist = (\n",
    "            sdf.groupBy(norm_expr)\n",
    "            .agg(F.count(\"*\").alias(\"count\"))\n",
    "            .orderBy(F.desc(\"count\"))\n",
    "        )\n",
    "\n",
    "        # Collect top categories (up to 100)\n",
    "        dist_rows = dist.limit(100).collect()\n",
    "        if not dist_rows:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            total_row = dist.agg(F.sum(\"count\").alias(\"total\")).first()\n",
    "            total = total_row[\"total\"]\n",
    "        except Exception:\n",
    "            total = None\n",
    "\n",
    "        top_values = []\n",
    "        for r in dist_rows:\n",
    "            value = r[\"value_norm\"]\n",
    "            count = int(r[\"count\"])\n",
    "            if total:\n",
    "                percent = float(count) / float(total)\n",
    "            else:\n",
    "                percent = None\n",
    "            top_values.append(\n",
    "                {\n",
    "                    \"value\": value,\n",
    "                    \"count\": count,\n",
    "                    \"percent\": percent,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            unique_values = dist.count()\n",
    "        except Exception:\n",
    "            unique_values = len(dist_rows)\n",
    "\n",
    "        categorical_profile[col] = {\n",
    "            \"unique_values\": int(unique_values),\n",
    "            \"top_values\": top_values,\n",
    "        }\n",
    "\n",
    "    # 4) Simple feature grouping by prefix\n",
    "    feature_groups = {}\n",
    "    for col_name in sdf.columns:\n",
    "        if \"_\" in col_name:\n",
    "            prefix = col_name.split(\"_\")[0]\n",
    "        else:\n",
    "            prefix = col_name\n",
    "        feature_groups.setdefault(prefix, []).append(col_name)\n",
    "\n",
    "    # 5) Save JSON for this dataset\n",
    "    out_obj = {\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"categorical_profile\": categorical_profile,\n",
    "        \"feature_groups\": feature_groups,\n",
    "    }\n",
    "\n",
    "    out_path = CAT_PROFILE_DIR / f\"{dataset_id}_categorical.json\"\n",
    "    with out_path.open(\"w\") as f:\n",
    "        json.dump(out_obj, f, indent=2)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    profiling_stats.append(\n",
    "        {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"status\": status,\n",
    "            \"error\": error,\n",
    "            \"seconds\": elapsed,\n",
    "            \"num_categorical_cols\": len(categorical_cols),\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Done. Profiling stats count:\", len(profiling_stats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a4740-78b1-4b6f-9d79-9e4658b3765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(profiling_stats)\n",
    "stats_df.to_csv(BASE_DIR / \"profiles\" / \"categorical_profiling_times.csv\", index=False)\n",
    "stats_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

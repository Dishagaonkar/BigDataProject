{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bafb276-e423-4d46-a0b3-1170b2fda03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Crawling full catalog for: 'Education' ---\n",
      "  Fetched batch: 693 items (Total so far: 693)\n",
      "\n",
      "--- Crawling full catalog for: 'City Government' ---\n",
      "  Fetched batch: 753 items (Total so far: 753)\n",
      "\n",
      "Total unique candidates found: 1409\n",
      "Checking row counts (this may take 1-2 minutes)...\n",
      "  Scanned 100/1409...\n",
      "  Scanned 200/1409...\n",
      "  Scanned 300/1409...\n",
      "  Scanned 400/1409...\n",
      "  Scanned 500/1409...\n",
      "  Scanned 600/1409...\n",
      "  Scanned 700/1409...\n",
      "  Scanned 800/1409...\n",
      "  Scanned 900/1409...\n",
      "  Scanned 1000/1409...\n",
      "  Scanned 1100/1409...\n",
      "  Scanned 1200/1409...\n",
      "  Scanned 1300/1409...\n",
      "  Scanned 1400/1409...\n",
      "\n",
      "\n",
      "=== FINAL DATASET LIST (> 100000 rows) ===\n",
      "Total Qualified Datasets: 94\n",
      "dataset_id  row_count                                                                name\n",
      " rmhc-afj9  391530043                                                 DSNY - PlowNYC Data\n",
      " wewp-mm3p  102269689                                             311 Call Center Inquiry\n",
      " a9md-ynri    6178555                                    Civil Service List Certification\n",
      " y7az-s7wc    4232279                           J-51 Exemption and Abatement (Historical)\n",
      " 797j-9xvg    3725986 NYC Historical Vital Records: Index to Digitized Death Certificates\n",
      " ye3c-m4ga    3237466                                                          Civil List\n",
      " xzj8-i3jk    3034726                        Asset Management Parks System (AMPS) – Labor\n",
      " rgyu-ii48    2788792                                       DOF Property Abatement Detail\n",
      " 5gq7-rgmv    2614956 NYC Historical Vital Records: Index to Digitized Birth Certificates\n",
      " cspg-yi7g    2403702                                 Construction Demolition Registrants\n",
      " vwpc-kje2    2269204                                          311 Web Content - Services\n",
      " irs3-wn2g    1659294                                            Vendor List by Commodity\n",
      " a8wp-rerh    1168957                                             Self Hauler Registrants\n",
      " 5zhs-2jue    1082999                                                            BUILDING\n",
      " d8dr-nyhw    1062855  NYC Historical Vital Records: Index to Digitized Marriage Licenses\n",
      " dg92-zbpx    1052564                                                  City Record Online\n",
      " mwzb-yiwb    1039025                                                      Expense Budget\n",
      " 5hjv-bjbv     947856                        New York City Seasonally Adjusted Employment\n",
      " dzvt-6g3v     907238                                           311 Interpreter Wait Time\n",
      " wavz-fkw8     904161                                            DOE Building Space Usage\n",
      "\n",
      "Saved full list to 'huge_datasets_list.csv'\n",
      "\n",
      "--- SAMPLE URLS FOR SPARK ---\n",
      "['https://data.cityofnewyork.us/api/views/rmhc-afj9/rows.csv?accessType=DOWNLOAD', 'https://data.cityofnewyork.us/api/views/wewp-mm3p/rows.csv?accessType=DOWNLOAD', 'https://data.cityofnewyork.us/api/views/a9md-ynri/rows.csv?accessType=DOWNLOAD', 'https://data.cityofnewyork.us/api/views/y7az-s7wc/rows.csv?accessType=DOWNLOAD', 'https://data.cityofnewyork.us/api/views/797j-9xvg/rows.csv?accessType=DOWNLOAD']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DOMAIN = \"data.cityofnewyork.us\"\n",
    "SEARCH_TERMS = [\"Education\", \"City Government\"]\n",
    "MIN_ROWS = 100000  # Threshold\n",
    "MAX_WORKERS = 10  # Number of parallel threads (don't go too high or API blocks you)\n",
    "\n",
    "def fetch_all_candidates(search_term):\n",
    "    \"\"\"\n",
    "    Crawls the Discovery API using 'limit' and 'offset' to get EVERYTHING.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Crawling full catalog for: '{search_term}' ---\")\n",
    "    url = \"http://api.us.socrata.com/api/catalog/v1\"\n",
    "    \n",
    "    candidates = []\n",
    "    offset = 0\n",
    "    limit = 2000 # Max allowed per page usually approx 2000-5000 depending on API version\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            'domains': DOMAIN,\n",
    "            'q': search_term,\n",
    "            'limit': limit,\n",
    "            'offset': offset,\n",
    "            'only': 'datasets'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            resp = requests.get(url, params=params).json()\n",
    "            results = resp.get('results', [])\n",
    "            \n",
    "            if not results:\n",
    "                break # No more results, stop looping\n",
    "                \n",
    "            candidates.extend(results)\n",
    "            print(f\"  Fetched batch: {len(results)} items (Total so far: {len(candidates)})\")\n",
    "            \n",
    "            offset += limit\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page: {e}\")\n",
    "            break\n",
    "            \n",
    "    return candidates\n",
    "\n",
    "def get_row_count_safe(dataset_meta):\n",
    "    \"\"\"\n",
    "    Worker function to check a single dataset's row count.\n",
    "    \"\"\"\n",
    "    d = dataset_meta['resource']\n",
    "    d_id = d['id']\n",
    "    name = d['name']\n",
    "    \n",
    "    url = f\"https://{DOMAIN}/resource/{d_id}.json?$select=count(*)\"\n",
    "    \n",
    "    try:\n",
    "        # Short timeout because we are doing many requests\n",
    "        r = requests.get(url, timeout=5)\n",
    "        if r.status_code == 200:\n",
    "            count = int(r.json()[0]['count'])\n",
    "            if count > MIN_ROWS:\n",
    "                return {\n",
    "                    'dataset_id': d_id,\n",
    "                    'row_count': count,\n",
    "                    'name': name,\n",
    "                    'download_url': f\"https://{DOMAIN}/api/views/{d_id}/rows.csv?accessType=DOWNLOAD\"\n",
    "                }\n",
    "    except:\n",
    "        return None # Fail silently to keep speed up\n",
    "    return None\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "all_candidates = []\n",
    "seen_ids = set()\n",
    "\n",
    "# 1. HARVEST ALL CANDIDATES\n",
    "for term in SEARCH_TERMS:\n",
    "    results = fetch_all_candidates(term)\n",
    "    for res in results:\n",
    "        res_id = res['resource']['id']\n",
    "        if res_id not in seen_ids:\n",
    "            seen_ids.add(res_id)\n",
    "            all_candidates.append(res)\n",
    "\n",
    "print(f\"\\nTotal unique candidates found: {len(all_candidates)}\")\n",
    "print(f\"Checking row counts (this may take 1-2 minutes)...\")\n",
    "\n",
    "# 2. PARALLEL PROBE\n",
    "valid_datasets = []\n",
    "\n",
    "# Using ThreadPool to check sizes concurrently\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all tasks\n",
    "    future_to_meta = {executor.submit(get_row_count_safe, item): item for item in all_candidates}\n",
    "    \n",
    "    # Process results as they complete\n",
    "    completed_count = 0\n",
    "    for future in concurrent.futures.as_completed(future_to_meta):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            valid_datasets.append(result)\n",
    "            # Optional: Print live updates for big finds\n",
    "            # print(f\"  [FOUND] {result['row_count']:>9,} rows | {result['name'][:40]}...\")\n",
    "        \n",
    "        completed_count += 1\n",
    "        if completed_count % 100 == 0:\n",
    "            print(f\"  Scanned {completed_count}/{len(all_candidates)}...\")\n",
    "\n",
    "# --- OUTPUT ---\n",
    "print(f\"\\n\\n=== FINAL DATASET LIST (> {MIN_ROWS} rows) ===\")\n",
    "print(f\"Total Qualified Datasets: {len(valid_datasets)}\")\n",
    "\n",
    "if valid_datasets:\n",
    "    df = pd.DataFrame(valid_datasets)\n",
    "    # Sort largest to smallest\n",
    "    df = df.sort_values(by='row_count', ascending=False)\n",
    "    \n",
    "    print(df[['dataset_id', 'row_count', 'name']].head(20).to_string(index=False))\n",
    "    \n",
    "    # Save to CSV for your comparison script\n",
    "    df.to_csv(\"huge_datasets_list.csv\", index=False)\n",
    "    print(\"\\nSaved full list to 'huge_datasets_list.csv'\")\n",
    "    \n",
    "    print(\"\\n--- SAMPLE URLS FOR SPARK ---\")\n",
    "    print(df['download_url'].head(5).tolist())\n",
    "else:\n",
    "    print(\"No datasets found matching criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da628ece-ad73-4d77-80db-baa9c0f7ff04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas 'line_terminator' patch applied.\n",
      "✅ 'datamart_profiler' library found. Using it for Local test.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/09 14:05:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing Profiling Speed on 10 datasets...\n",
      "Dataset Name                   | Rows       | Local (s)  | Spark (s)  | Ratio\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/datamart_profiler/core.py:985: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  sample = sample.applymap(truncate_string)  # Truncate long values\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC Historical Vital Records   | 3725986    | 36.60      | 455.32     | 12.4x Slower\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/datamart_profiler/core.py:985: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  sample = sample.applymap(truncate_string)  # Truncate long values\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Civil List                     | 3237466    | 23.79      | 227.98     | 9.6x Slower\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/datamart_profiler/core.py:985: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  sample = sample.applymap(truncate_string)  # Truncate long values\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asset Management Parks Syste   | 3034726    | 37.86      | 443.03     | 11.7x Slower\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/datamart_profiler/core.py:985: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  sample = sample.applymap(truncate_string)  # Truncate long values\n",
      "[Stage 19:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import tempfile\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.core.generic import NDFrame\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# 1. Fix the Recursion Error\n",
    "sys.setrecursionlimit(5000)  # Increase from default 1000 to 5000\n",
    "\n",
    "# 1. Save the original to_csv function\n",
    "_original_to_csv = NDFrame.to_csv\n",
    "\n",
    "# 2. Define a new wrapper function that fixes the argument\n",
    "def _patched_to_csv(self, *args, **kwargs):\n",
    "    # If the old argument is present, swap it for the new one\n",
    "    if 'line_terminator' in kwargs:\n",
    "        kwargs['lineterminator'] = kwargs.pop('line_terminator')\n",
    "    \n",
    "    # Call the original function with the fixed arguments\n",
    "    return _original_to_csv(self, *args, **kwargs)\n",
    "\n",
    "# 3. Apply the patch\n",
    "NDFrame.to_csv = _patched_to_csv\n",
    "print(\"Pandas 'line_terminator' patch applied.\")\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATASET_LIST_FILE = \"huge_datasets_list.csv\"  # The list you generated earlier\n",
    "OUTPUT_FILE = \"profiler_benchmark_results.csv\"\n",
    "\n",
    "# Try to import the REAL Datamart Profiler (NYU Library)\n",
    "try:\n",
    "    import datamart_profiler\n",
    "    HAS_DATAMART = True\n",
    "    print(\"✅ 'datamart_profiler' library found. Using it for Local test.\")\n",
    "except ImportError:\n",
    "    HAS_DATAMART = False\n",
    "    print(\"⚠️ 'datamart_profiler' not found. Using Pandas .describe() as valid proxy.\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOCAL PROFILING (The Baseline)\n",
    "# ==========================================\n",
    "def run_local_profiler(download_url):\n",
    "    \"\"\"\n",
    "    Simulates the standard AutoDDG flow:\n",
    "    Download -> Save -> Run datamart_profiler (or Pandas) -> Load Stats\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Download to a temp file (Local tools usually need a file on disk)\n",
    "        # We time the download because local tools CANNOT stream efficiently like Spark\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp:\n",
    "            r = requests.get(download_url, stream=True)\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                tmp.write(chunk)\n",
    "            tmp_path = tmp.name\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if HAS_DATAMART:\n",
    "            # --- THE REAL DATAMART PROFILER ---\n",
    "            # This is exactly what the original AutoDDG likely does\n",
    "            metadata = datamart_profiler.process_dataset(tmp_path, include_sample=True)\n",
    "        else:\n",
    "            # --- PANDAS PROXY ---\n",
    "            # Approximates the work: Load whole file -> Calc Stats\n",
    "            df = pd.read_csv(tmp_path)\n",
    "            stats = df.describe(include='all')\n",
    "            # Simulate \"Type Detection\" (iterating columns)\n",
    "            dtypes = df.dtypes.to_dict()\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        # Cleanup\n",
    "        os.remove(tmp_path)\n",
    "        return duration\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [Local Fail] {e}\")\n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# 2. SPARK PROFILING (The Scalable Solution)\n",
    "# ==========================================\n",
    "def run_spark_profiler(spark, download_url):\n",
    "    \"\"\"\n",
    "    Simulates the Scalable flow:\n",
    "    Stream Read -> Distributed Compute -> Collect Stats\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Spark reads directly (Lazy Evaluation)\n",
    "        # Note: We include reading in the time, but Spark streams it.\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # For simplicity in this script, we assume Spark can access the URL.\n",
    "        # If your Spark is local, we might need to download first to be fair, \n",
    "        # BUT the advantage of Spark is streaming. \n",
    "        # Let's download to temp to give a fair \"Apples to Apples\" on PROCESSING speed.\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp:\n",
    "            r = requests.get(download_url, stream=True)\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                tmp.write(chunk)\n",
    "            tmp_path = tmp.name\n",
    "\n",
    "        # --- THE SPARK WORK ---\n",
    "        df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(tmp_path)\n",
    "        new_columns = [c.replace('.', '_') for c in df.columns]\n",
    "        df = df.toDF(*new_columns)\n",
    "        \n",
    "        # Force computation of the profile\n",
    "        # .summary() computes count, mean, stddev, min, max, etc.\n",
    "        summary_stats = df.summary().collect()\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        os.remove(tmp_path)\n",
    "        return duration\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [Spark Fail] {e}\")\n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# 3. RUN COMPARISON\n",
    "# ==========================================\n",
    "def main():\n",
    "    # Load your list of datasets\n",
    "    if not os.path.exists(DATASET_LIST_FILE):\n",
    "        print(\"Please run the 'Deep Scan' script first to generate the dataset list.\")\n",
    "        return\n",
    "\n",
    "    df_datasets = pd.read_csv(DATASET_LIST_FILE)[4:14] # Test top 10 first\n",
    "    \n",
    "    # Init Spark\n",
    "    spark = SparkSession.builder.appName(\"ProfilerBenchmark\").master(\"local[*]\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nComparing Profiling Speed on {len(df_datasets)} datasets...\")\n",
    "    print(f\"{'Dataset Name':<30} | {'Rows':<10} | {'Local (s)':<10} | {'Spark (s)':<10} | {'Ratio'}\")\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    for _, row in df_datasets.iterrows():\n",
    "        url = row['download_url']\n",
    "        name = row['name'][:28]\n",
    "        rows = row['row_count']\n",
    "        \n",
    "        # Run Local\n",
    "        t_local = run_local_profiler(url)\n",
    "        \n",
    "        # Run Spark\n",
    "        t_spark = run_spark_profiler(spark, url)\n",
    "        \n",
    "        # Calculate Speedup\n",
    "        ratio = \"N/A\"\n",
    "        if t_local and t_spark:\n",
    "            if t_spark < t_local:\n",
    "                ratio = f\"{t_local / t_spark:.1f}x Faster\"\n",
    "            else:\n",
    "                ratio = f\"{t_spark / t_local:.1f}x Slower\" # Happens on small data\n",
    "\n",
    "                    # Format the values safely beforehand\n",
    "            s_local = f\"{t_local:.2f}\" if isinstance(t_local, (int, float)) else \"Err\"\n",
    "            s_spark = f\"{t_spark:.2f}\" if isinstance(t_spark, (int, float)) else \"Err\"\n",
    "            \n",
    "            # Print using string formatting (<10s is implicit for strings)\n",
    "            print(f\"{name:<30} | {rows:<10} | {s_local:<10} | {s_spark:<10} | {ratio}\")\n",
    "                    \n",
    "        results.append({\n",
    "            'dataset': name,\n",
    "            'rows': rows,\n",
    "            'time_local': t_local,\n",
    "            'time_spark': t_spark\n",
    "        })\n",
    "\n",
    "    # Save\n",
    "    pd.DataFrame(results).to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nResults saved to {OUTPUT_FILE}\")\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed0c1a-9aaa-4a73-8d38-6ada49e1138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas 'line_terminator' patch applied.\n",
      "✅ 'datamart_profiler' library found. Using it for Local test.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/09 15:26:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing Profiling Speed on Top 4 Datasets (Ascending Order)...\n",
      "Dataset Name                   | Rows       | Local (s)  | Spark (s)  | Ratio\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/datamart_profiler/core.py:985: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  sample = sample.applymap(truncate_string)  # Truncate long values\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J-51 Exemption and Abatement   | 4232279    | Err        | 113.71     | Spark Won\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/datamart_profiler/core.py:985: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  sample = sample.applymap(truncate_string)  # Truncate long values\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Civil Service List Certifica   | 6178555    | Err        | 764.51     | Spark Won\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/datamart_profiler/core.py:985: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  sample = sample.applymap(truncate_string)  # Truncate long values\n",
      "[Stage 12:==>                                                     (6 + 2) / 155]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import tempfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pandas.core.generic import NDFrame\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# --- 1. PATCHES & CONFIG ---\n",
    "\n",
    "# Fix Recursion Error for complex datasets\n",
    "sys.setrecursionlimit(5000)\n",
    "\n",
    "# Fix Pandas 'line_terminator' Compatibility\n",
    "_original_to_csv = NDFrame.to_csv\n",
    "\n",
    "def _patched_to_csv(self, *args, **kwargs):\n",
    "    if 'line_terminator' in kwargs:\n",
    "        kwargs['lineterminator'] = kwargs.pop('line_terminator')\n",
    "    return _original_to_csv(self, *args, **kwargs)\n",
    "\n",
    "NDFrame.to_csv = _patched_to_csv\n",
    "print(\"Pandas 'line_terminator' patch applied.\")\n",
    "\n",
    "# Configuration\n",
    "DATASET_LIST_FILE = \"huge_datasets_list.csv\"\n",
    "OUTPUT_FILE = \"profiler_benchmark_results_top4.csv\"\n",
    "\n",
    "# Check for Datamart Profiler\n",
    "try:\n",
    "    import datamart_profiler\n",
    "    HAS_DATAMART = True\n",
    "    print(\"✅ 'datamart_profiler' library found. Using it for Local test.\")\n",
    "except ImportError:\n",
    "    HAS_DATAMART = False\n",
    "    print(\"⚠️ 'datamart_profiler' not found. Using Pandas .describe() as valid proxy.\")\n",
    "\n",
    "# --- 2. PROFILER FUNCTIONS ---\n",
    "\n",
    "def run_local_profiler(download_url):\n",
    "    try:\n",
    "        # Download to temp file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp:\n",
    "            r = requests.get(download_url, stream=True)\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                tmp.write(chunk)\n",
    "            tmp_path = tmp.name\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if HAS_DATAMART:\n",
    "            metadata = datamart_profiler.process_dataset(tmp_path, include_sample=True)\n",
    "        else:\n",
    "            df = pd.read_csv(tmp_path)\n",
    "            stats = df.describe(include='all')\n",
    "            dtypes = df.dtypes.to_dict()\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        os.remove(tmp_path)\n",
    "        return duration\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Don't print full error to keep table clean, just log failure\n",
    "        return None\n",
    "\n",
    "def run_spark_profiler(spark, download_url):\n",
    "    try:\n",
    "        # Download to temp file (for fair comparison of processing time)\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp:\n",
    "            r = requests.get(download_url, stream=True)\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                tmp.write(chunk)\n",
    "            tmp_path = tmp.name\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Spark Read\n",
    "        df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(tmp_path)\n",
    "        \n",
    "        # Fix column names (replace dots with underscores)\n",
    "        new_columns = [c.replace('.', '_') for c in df.columns]\n",
    "        df = df.toDF(*new_columns)\n",
    "        \n",
    "        # Force computation\n",
    "        summary_stats = df.summary().collect()\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        os.remove(tmp_path)\n",
    "        return duration\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[Spark Error] {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 3. MAIN EXECUTION ---\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(DATASET_LIST_FILE):\n",
    "        print(\"Dataset list file not found.\")\n",
    "        return\n",
    "\n",
    "    # Load and Prepare the \"Top 4 Ascending\" List\n",
    "    df = pd.read_csv(DATASET_LIST_FILE)\n",
    "    datasets = df.to_dict('records') # Convert to list of dicts for easier sorting\n",
    "\n",
    "    # Step A: Sort Descending to find the Giants (Using 'row_count'!)\n",
    "    all_sorted_desc = sorted(datasets, key=lambda x: x['row_count'], reverse=True)\n",
    "    \n",
    "    # Step B: Take the Top 4 Giants\n",
    "    top_4_monsters = all_sorted_desc[:4]\n",
    "    \n",
    "    # Step C: Sort them Ascending (Smallest Giant first)\n",
    "    final_list = sorted(top_4_monsters, key=lambda x: x['row_count'])\n",
    "\n",
    "    # Init Spark\n",
    "    spark = SparkSession.builder.appName(\"ProfilerBenchmark\").master(\"local[*]\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\") # Hide progress bars\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nComparing Profiling Speed on Top 4 Datasets (Ascending Order)...\")\n",
    "    print(f\"{'Dataset Name':<30} | {'Rows':<10} | {'Local (s)':<10} | {'Spark (s)':<10} | {'Ratio'}\")\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    for row in final_list:\n",
    "        url = row['download_url']\n",
    "        name = row['name'][:28] # Truncate name\n",
    "        rows = row['row_count'] # <--- Fixed Key Here\n",
    "        \n",
    "        # 1. Run Local\n",
    "        t_local = run_local_profiler(url)\n",
    "        \n",
    "        # 2. Run Spark\n",
    "        t_spark = run_spark_profiler(spark, url)\n",
    "        \n",
    "        # 3. Calculate Ratio and Format Strings\n",
    "        s_local = f\"{t_local:.2f}\" if t_local is not None else \"Err\"\n",
    "        s_spark = f\"{t_spark:.2f}\" if t_spark is not None else \"Err\"\n",
    "        \n",
    "        ratio = \"N/A\"\n",
    "        if t_local and t_spark:\n",
    "            if t_spark < t_local:\n",
    "                ratio = f\"{t_local / t_spark:.1f}x Faster\"\n",
    "            else:\n",
    "                ratio = f\"{t_spark / t_local:.1f}x Slower\"\n",
    "        elif t_local is None and t_spark is not None:\n",
    "            ratio = \"Spark Won\"\n",
    "        elif t_local is not None and t_spark is None:\n",
    "            ratio = \"Local Won\"\n",
    "\n",
    "        # 4. Print Row\n",
    "        print(f\"{name:<30} | {rows:<10} | {s_local:<10} | {s_spark:<10} | {ratio}\")\n",
    "                \n",
    "        results.append({\n",
    "            'dataset': name,\n",
    "            'rows': rows,\n",
    "            'time_local': t_local,\n",
    "            'time_spark': t_spark\n",
    "        })\n",
    "\n",
    "    # Save\n",
    "    pd.DataFrame(results).to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nResults saved to {OUTPUT_FILE}\")\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
